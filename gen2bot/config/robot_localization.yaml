# Look at: http://docs.ros.org/en/noetic/api/robot_localization/html/index.html#:~:text=robot_localization%20is%20a%20collection%20of,estimation%20nodes%2C%20ekf_localization_node%20and%20ukf_localization_node%20.
# and https://kapernikov.com/the-ros-robot_localization-package/
# and http://docs.ros.org/en/melodic/api/robot_localization/html/state_estimation_nodes.html
# to understand parameters and purpose of robot_localization ekf

# Ok, so we need both absolute and relative postion for robot. Absolute is based off map's fixed frame
# and relative is based off odom's fixed frame. Absolute typically uses GPS, SLAM, etc. for discrete 
# (big changes in position) updates and relative uses IMU, visual odometry, and wheel encoders for continous 
# (small changes in position) updates. Intel's algorithm does both continuous and discrete, so we need to make 2 
# robot_localization nodes that publishes both map to odom (discrete) and odom to base_link (continuous)

# in hertz, how quick to update pose estimation of base_link, might need to update to like 240 for gyro sensor
frequency: 60

# makes the pose estimate expand into the Z axis, which is required for our robot that will
# move on hills and craters and bumps and such
# will test how it works when true, might save tons of computational power for no big cost
two_d_mode: true

# Define tf frames for static map, odom_frame (fixed frame where robot spawns in), and robot's frame
# if world_frame = odom, publishes odom to base_link, if world_frame = map, publishes map to odom
world_frame: camera_pose_frame
map_frame: map
odom_frame: camera_pose_frame
base_link_frame: base_link

# imu gets change in velocity (acceleration) to record position
# imu topic to subscribe to
imu0: /tracking_camera/imu

# I don't know which ones T265 IMU uses, so we have to figure out which ones we want and 
# disable the other ones or figure out if the ones not used take up any processing power,
# thus making us able to not waste time figuring out which should be true or false
imu0_config: [true, true, true,
               true, true, true,
               true, true, true,
               true, true, true,
               true, true, true]

# differential gets linear and turn velocities relative to last known time (ex. wheel encoders)
imu0_differential: false

# relative gets pose and orientation relative to original position (ex. GPS or SLAM or visual imu odometry)
imu0_relative: true

# odometry also gets pose estimation, usually with wheels
odom0: /tracking_camera/odom/sample

# we also have to find which ones should be true or false for these
odom0_config: [true, true, true,
               true, true, true,
               true, true, true,
               true, true, true,
               true, true, true]

# Both of these can't be true
odom0_differential: true
odom0_relative: true


# From twistConvertor.py
#twist0: /twist_with_covariance1


#Translation in X and Y and turn in yaw?
#twist0_config: [false, false, false,
#                false, false, false,
 #               true,  true,  true,
#                false, false, false,
 #               false, false, false]
#twist0_queue_size: 1
#twist0_differential: true
#twist0_relative: false

# From twistConvertor.py
#twist1: /twist_with_covariance2
#twist1_config: [false, false, false,
#                false, false, false,
#                true,  true,  true,
#                false, false, false,
#                false, false, false]
#twist1_queue_size: 1
#twist1_differential: true
#twist1_relative: false


# Publishes change in position of either odom to robot or map to odom, we have it for odom to robot
# based on what we set as the world frame
publish_tf: true

# gives range of time tf can call upon, so if it takes too long to publish tfs, it will pull from
# past tfs or push its tf into the future by given seconds
tf_tolerance: 0.5
