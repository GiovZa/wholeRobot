<launch>
<!-- TEST OUT IF THESE CHANGES DON'T BREAK CODE: group ns= "camera" changed to only include depth camera, respawn parameter 
removed in rviz node, line 41: <include file="$(find gen2bot)launch/rgbd.launch"> the / is added -->
<!-- Learn about launch files in ros here: http://wiki.ros.org/roslaunch/XML -->

<!-- This launch file currently opens both camera nodes, rviz, nodes that allow pointcloud, a map server that detects obstacles
, QR code reconition node, robot_localization that updates position of robot, static transforms, joystick implementation, motor
commands, and move_base, which finds a path to desired goal-->

<!-- Currently, we need to split these nodes into different launch files so each computer can run its own nodes, not all on the jetson -->

<!-- Launches depth camera with following parameters: 
  https://github.com/IntelRealSense/realsense-ros/tree/ros1-legacy -->

  <!-- This group ns is what shows up in ros graph, making almost everything go inside this camera umbrella, will need to rework structure-->
  <group ns="camera">

  <!--This file can be found in Other Locations/opt/ros/melodic/share/launch/includes on the jetson-->
    <include file="$(find realsense2_camera)/launch/includes/nodelet.launch.xml">

  <!-- Some of these parameters are in default settings, meaning we don't need to declare them, but leave
  for now as we might need to change these for our next object recognition script-->
      <arg name="depth_width"       value="640"/>
      <arg name="depth_height"      value="480"/>
      <arg name="depth_fps"         value="30"/>
      <arg name="color_width"       value="640"/>
      <arg name="color_height"      value="480"/>
      <arg name="color_fps"         value="30"/>
      <arg name="enable_fisheye"    value="false"/>
      <arg name="enable_pointcloud" value="false"/>
      <arg name="enable_sync"       value="true"/>
      <arg name="enable_gyro"       value="false"/>
      <arg name="enable_accel"      value="false"/>
      <arg name="enable_imu"        value="false"/>
      <arg name="tf_prefix"         value="camera"/>
      <arg name="align_depth"       value="true"/>
    </include>
  </group>

<!-- Launches rviz using the rgbd.launch file, opens 3 nodes that seems to not send out topics but if I get rid of them, pointcloud
disappears which is not good, maybe someone figures out why these nodes are needed? The file is in this directory, rgbd.launch -->
    <node name="rviz" pkg="rviz" type="rviz" args="-d $(find realsense2_camera)/rviz/pointcloud.rviz" required="true" />
    <include file="$(find gen2bot)/launch/rgbd.launch">
      <arg name="manager"                       value="realsense2_camera_manager" />
      <arg name="rgb"                           value="color" />
    </include>

<!-- Remaps pointcloud so that it subscribes to a topic possible for jetson to visualize, without 
this node, jetson won't publish pointcloud, which navigation and qr code relies on -->
      <node pkg="nodelet" type="nodelet" name="points_xyzrgb_hw_registered"
            args="load depth_image_proc/point_cloud_xyzrgb realsense2_camera_manager false" respawn="false">
        <!-- Need these to make a pointcloud-->
        <remap from="rgb/image_rect_color"        to="color/image_rect_color" />
        <remap from="rgb/camera_info"             to="color/camera_info" />
        <remap from="depth_registered/image_rect" to="aligned_depth_to_color/image_raw" />
        <remap from="depth_registered/points"     to="/camera/depth/color/points" />
      </node>

<!-- Map server used to create a map that detects obstacles check out: 
  http://wiki.ros.org/octomap_server for parameter list
  this detects obstacles for global planner's static frame: map-->
	<node pkg="octomap_server" type="octomap_server_node" name="octomap_server">

  <!-- The frame where ground level is detected-->
		<param name="base_frame_id" type="string" value="/base_footprint" />

  <!-- The frame where obstacles are embedded onto-->
		<param name="frame_id" type="string" value="/map" />

  <!-- The max range in which cameras can detect an obstacle-->
		<param name="sensor_model/max_range" value="3.0" />

  <!-- Enables detection of a ground plane, so we ignore the ground. 
  Otherwise, whole map is an obstacle-->
		<param name="filter_ground" type="bool" value="true" />

  <!-- How much in meters in Z direction to segment voxels into ground plane-->
    <param name="ground_filter/plane_distance" value="0.2" />
  
  <!-- The minimum and maximum range in which obstacles can be detected relative to camera-->
		<param name="pointcloud_min_z" value="-0.90" />
		<param name="pointcloud_max_z" value="0.10" />

  <!-- Remaps subscribed topic to topic in which pointcloud is published-->
		<remap from="cloud_in" to="/camera/depth/color/points" />

  <!-- Remaps published topic to /map so map frame gets obstacles-->
   	<remap from="projected_map" to="/map"/>

    <remap from="rgb/image_rect_color"        to="color/image_rect_color" />
    <remap from="rgb/camera_info"             to="color/camera_info" />
    <remap from="depth_registered/image_rect" to="aligned_depth_to_color/image_raw" />
    <remap from="depth_registered/points"     to="/camera/depth/color/points" />
	</node>

<!-- Launches tracking camera with following parameters: 
  https://github.com/IntelRealSense/realsense-ros/tree/ros1-legacy -->
    <group ns="camera">
      <include file="$(find realsense2_camera)/launch/includes/nodelet.launch.xml">
        <!-- Initializes T265 camera-->
        <arg name="tf_prefix"                value="camera"/>
        <arg name="device_type"              value="t265"/>

        <!-- Enables IMU sensors-->
        <arg name="enable_gyro"              value="true"/>
        <arg name="enable_accel"             value="true"/>
        <arg name="enable_pose"              value="true"/>
       
        <!-- Syncs both IMU sensors into 1 sensor-->
        <arg name="unite_imu_method"         value="linear_interpolation"/>

        <!-- Sets up the global static frame "map" and gives pose estimation
        between map and camera_pose_frame (global -> odom) -->
        <arg name="publish_odom_tf"          value="true"/>
        <arg name="publish_tf"               value="true"/>
        <arg name="odom_frame_id"            value="map"/> 
      </include>
    </group>
 
<!-- Object recognition package to detect QR code and display both position and orientation 
  of QR code relative to camera sensor
  Check out: https://github.com/bandasaikrishna/object_detection_and_3d_pose_estimation -->
  <!-- Will need to decide if we want to keep the gui or not, gives us camera image but takes up space and processing power-->

  <node name="find_object_3d" pkg="find_object_2d" type="find_object_2d" output="screen">
		<param name="settings_path" value="~/.ros/find_object_2d.ini" type="str"/>
		
    <!-- Where to store QR code image-->
		<param name="session_path" value="$(find gen2bot)/sessions/qr.bin" type="str"/>
		
		<remap from="rgb/image_rect_color" to="/camera/color/image_raw"/>
		<remap from="depth_registered/image_raw" to="/camera/depth/image_rect_raw"/>
		<remap from="depth_registered/camera_info" to="/camera/depth/camera_info"/>
	</node>
	
	<node name="tf_example" pkg="find_object_2d" type="tf_example" output="screen">
	    <param name="map_frame_id" value="/camera" type="str"/>
		<param name="object_prefix" value="object" type="str"/>
	</node> 

<!-- Look at: http://docs.ros.org/en/noetic/api/robot_localization/html/index.html#:~:text=robot_localization%20is%20a%20collection%20of,estimation%20nodes%2C%20ekf_localization_node%20and%20ukf_localization_node%20.
and https://kapernikov.com/the-ros-robot_localization-package/
and http://docs.ros.org/en/melodic/api/robot_localization/html/state_estimation_nodes.html
to understand parameters and purpose of robot_localization ekf -->
<!-- Publishes base_link tranform from camera_pose_frame using VIO, and move_base twist topic-->
<node pkg="robot_localization" type="ekf_localization_node" name="ekf_localization" output="screen">
    <rosparam file="$(find gen2bot)/config/robot_localization.yaml" command="load"/>
</node>

<!-- Static tranform between parent and child transforms. Arguments are position for x, y, and z followed by orientation for x, y, and z -->
<!-- These nodes just sticks a child tf to follow a parent tf a certain position and orientation away -->
<node pkg="tf" type="static_transform_publisher" name="base_link_to_base_footprint" args= "-0.40 0.15 -0.45 0 0 0 base_link base_footprint 100"/>

<!-- Launches move_base which spews out motor commands based on nav_goal and obstacles
  Check out: http://wiki.ros.org/move_base for parameters -->
<node pkg="move_base" type="move_base" respawn="false" name="move_base" output="screen"> 
	
    <!-- Decrease turn velocty so robot doesn't snap into a 180 and break itself-->
    <param name="max_vel_theta" value="0.33" type="double"/>
    <param name="min_vel_theta" value="-0.33" type="double"/>
    <param name="controller_frequency" value="6.0" type="double"/>

<!-- Sets common_costmap parameters in config folder to both local and global costmaps -->
    <rosparam file="$(find gen2bot)/config/common_costmap.yaml" command="load" ns="global_costmap" />
    <rosparam file="$(find gen2bot)/config/common_costmap.yaml" command="load" ns="local_costmap" />
    
<!-- Sets local and global parameters in config folder to both local and global costmaps -->
    <rosparam file="$(find gen2bot)/config/global_costmap.yaml" command="load" ns="global_costmap" /> 
    <rosparam file="$(find gen2bot)/config/local_costmap.yaml" command="load" ns="local_costmap" />
</node>

<node name="move_base_deposit_client" pkg="gen2bot" type="move_base_deposit_client.py" output="screen">
</node>

<node name="move_base_dig_client" pkg="gen2bot" type="move_base_dig_client.py" output="screen">
</node>

<!-- Launches my listenerMotorAuto.cpp file to initialize a base controller 
  that move_base communicates with to send motor inputs. This links autonomy to wheel motors -->
<node pkg="gen2bot" type="listenerMotorAuto" name="autoWheels" >
    <param name="initalOutput" value="0.0" type="double"/>
</node>

<!-- Launches my move_base_client.py script to send desired positions for robot to reach to move_base planner->
<node name="move_baseClientObject22" pkg="gen2bot" type="move_base_client.py" output="screen">
</node> -->

<!-- Launches node that converts xbox controller inputs into ROS-->
 <node name="joy2" pkg="joy" type="joy_node" output="screen">

    <!-- parameter that retrieves which joystick controller to link with -->
    <param name="dev" value="/dev/input/js0"/>
    <param name="default_trig_val" value="true"/>
 </node>
 
 <!-- Node that takes ROS inputs of joystick to move motors in wheels, auger, ballscrew, and linear actuator-->
 <node name="manualMotorControl" pkg="gen2bot" type="notDTTalker.py" output="screen">
 </node>

<!-- Takes in twist messages and converts them to twistWithCovariancePose messages so robot_localization can use them-->
  <node name="twistConvertor" pkg="gen2bot" type="twistConvertor.py" output="screen">
 </node>

 <!-- Node that takes ROS inputs of joystick to move motors in wheels-->
 <node name="wheelies" pkg="gen2bot" type="listenerMotor" output="screen">
 </node>

<!-- Node that contains all motor functions that don't relate to driving -->
 <node pkg="gen2bot" type="notDT" name="notDT" output="screen">
    <param name="wheel_multiplier" value="1.1" type="double"/>
    <rosparam file="$(find gen2bot)/config/PID.yaml" command="load"/> 
 </node> 

<!-- loads states.yaml's parameters-->
<rosparam file="$(find gen2bot)/config/states.yaml" command="load"/> 

</launch>
