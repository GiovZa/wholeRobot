<launch>
<!-- Learn about launch files in ros here: http://wiki.ros.org/roslaunch/XML -->

<!-- This launch file is the poorly put together code that will run on 03/24/23 for NASA's proof of life video-->

<!-- Currently, we need to split these nodes into different launch files so each computer can run its own nodes, not all on the jetson -->

<!-- Launches depth camera with following parameters: 
  https://github.com/IntelRealSense/realsense-ros/tree/ros1-legacy -->

  <!-- This group ns is what shows up in ros graph, making almost everything go inside this camera umbrella, will need to rework structure-->
  <group ns="depth_camera">

  <!--This file can be found in Other Locations/opt/ros/melodic/share/launch/includes on the jetson-->
    <include file="$(find realsense2_camera)/launch/includes/nodelet.launch.xml">

  <!-- Some of these parameters are in default settings, meaning we don't need to declare them, but leave
  for now as we might need to change these for our next object recognition script-->
      <arg name="depth_width"       value="640"/>
      <arg name="depth_height"      value="480"/>
      <arg name="depth_fps"         value="30"/>
      <arg name="color_width"       value="640"/>
      <arg name="color_height"      value="480"/>
      <arg name="color_fps"         value="30"/>
      <arg name="enable_fisheye"    value="false"/>
      <arg name="enable_pointcloud" value="false"/>
      <arg name="enable_sync"       value="true"/>
      <arg name="enable_gyro"       value="false"/>
      <arg name="enable_accel"      value="false"/>
      <arg name="tf_prefix"         value="camera"/>
      <arg name="align_depth"       value="true"/>
    </include>

<!-- Launches rviz using the rgbd.launch file, opens 3 nodes that seems to not send out topics but if I get rid of them, pointcloud
disappears which is not good, maybe someone figures out why these nodes are needed? The file is in this directory, rgbd.launch -->
    <node name="rviz" pkg="rviz" type="rviz" args="-d $(find realsense2_camera)/rviz/pointcloud.rviz" required="true" />
      <include file="$(find gen2bot)/launch/rgbd.launch">
        <arg name="manager"                       value="realsense2_camera_manager" />
        <arg name="respawn"                       value="false" />      
        <arg name="rgb"                           value="color" />
      </include>

<!-- Remaps pointcloud so that it subscribes to a topic possible for jetson to visualize, without 
this node, jetson won't publish pointcloud, which navigation and qr code relies on -->
    <node pkg="nodelet" type="nodelet" name="points_xyzrgb_hw_registered"
      args="load depth_image_proc/point_cloud_xyzrgb realsense2_camera_manager false" respawn="false">
     
      <!-- Need these to make a pointcloud-->
      <remap from="rgb/image_rect_color"        to="color/image_rect_color" />
      <remap from="rgb/camera_info"             to="color/camera_info" />
      <remap from="depth_registered/image_rect" to="aligned_depth_to_color/image_raw" />
      <remap from="depth_registered/points"     to="/depth_camera/depth/color/points" />
    </node>
  </group>

<!-- Launches tracking camera with following parameters: 
  https://github.com/IntelRealSense/realsense-ros/tree/ros1-legacy -->
  <group ns="tracking_camera">
    <include file="$(find realsense2_camera)/launch/includes/nodelet.launch.xml">
      <!-- Initializes T265 camera-->
      <arg name="tf_prefix"                value="camera"/>
      <arg name="device_type"              value="t265"/>

      <!-- Enables IMU sensors-->
      <arg name="enable_gyro"              value="true"/>
      <arg name="enable_accel"             value="true"/>
      <arg name="enable_pose"              value="true"/>
      
      <!-- Syncs both IMU sensors into 1 sensor-->
      <arg name="unite_imu_method"         value="linear_interpolation"/>

      <!-- Sets up the global static frame "map" and gives pose estimation
      between map and camera_pose_frame (global -> odom) -->
      <arg name="publish_odom_tf"          value="true"/>
      <arg name="publish_tf"               value="true"/>
    </include>
  </group>

  <!-- Object recognition package to detect QR code and display both position and orientation 
  of QR code relative to camera sensor
  Check out: https://github.com/bandasaikrishna/object_detection_and_3d_pose_estimation -->
  <!-- Will need to decide if we want to keep the gui or not, gives us camera image but takes up space and processing power-->
  <!--Parameter list: http://wiki.ros.org/find_object_2d-->
  <group ns = "qr_code_detector">
    <node name="find_object_3d" pkg="find_object_2d" type="find_object_2d" output="screen">
      <param name="settings_path" value="~/.ros/find_object_2d.ini" type="str"/>
      <param name="subscribe_depth" value="true" type="bool"/>
      <param name="gui" value="false" type="bool"/>

      <!-- Where to store QR code image-->
      <param name="session_path" value="$(find gen2bot)/sessions/qr.bin" type="str"/>
      
      <!-- Remap image topic to one the depth camera uses-->
      <remap from="rgb/image_rect_color" to="/depth_camera/color/image_raw"/>
      <remap from="depth_registered/image_raw" to="/depth_camera/depth/image_rect_raw"/>
      <remap from="depth_registered/camera_info" to="/depth_camera/depth/camera_info"/>
    </node>
    
    <node name="tf_example" pkg="find_object_2d" type="tf_example" output="screen">
        <param name="map_frame_id" value="/camera" type="str"/>
      <param name="object_prefix" value="object" type="str"/>
    </node> 
  </group>


  <!-- Map server used to create a map that detects obstacles check out: 
  http://wiki.ros.org/octomap_server for parameter list
  this detects obstacles for global planner's static frame: map-->
	<node pkg="octomap_server" type="octomap_server_node" name="octomap_server">

  <!-- The frame where ground level is detected-->
		<param name="base_frame_id" type="string" value="/base_footprint" />

  <!-- The frame where obstacles are embedded onto-->
		<param name="frame_id" type="string" value="/map" />

  <!-- The max range in which cameras can detect an obstacle-->
		<param name="sensor_model/max_range" value="3.0" />

  <!-- Enables detection of a ground plane, so we ignore the ground. 
  Otherwise, the whole ground is an obstacle-->
		<param name="filter_ground" type="bool" value="true" />

  <!-- How much in meters in Z direction to segment voxels into ground plane-->
    <param name="ground_filter/plane_distance" value="0.2" />
  
  <!-- The minimum and maximum range in which obstacles can be detected relative to camera-->
		<param name="pointcloud_min_z" value="-0.35" />
		<param name="pointcloud_max_z" value="0.10" />

  <!-- Remaps subscribed topic to topic in which pointcloud is published-->
		<remap from="cloud_in" to="/depth_camera/depth/color/points" />

  <!-- Remaps published topic to /map so map frame gets obstacles-->
   	<remap from="projected_map" to="/map"/>
	</node>

  <!-- Static tranform between parent and child transforms. Arguments are position for x, y, and z followed by orientation for x, y, and z -->
  <!-- These nodes just sticks a child tf to follow a parent tf a certain position and orientation away -->
  <node pkg="tf" type="static_transform_publisher" name="base_link_to_base_footprint" args= "-0.40 0.15 -0.45 0 0 0 base_link base_footprint 100"/>
  <node pkg="tf" type="static_transform_publisher" name="camera_pose_to_base_link" args= "0.0 0.0 0.0 0 0 0 camera_pose_frame base_link 100"/>
  <node pkg="tf" type="static_transform_publisher" name="map_to_camera_odom" args= "-0.30 0.0 -0.15 0 0 0 camera_odom_frame map 100"/>
  
  <!-- Launches move_base which spews out motor commands based on nav_goal and obstacles
    Check out: http://wiki.ros.org/move_base for parameters -->
  <node pkg="move_base" type="move_base" respawn="false" name="move_base" output="screen"> 
    
    <!-- Decrease turn velocty so robot doesn't snap into a 180 and break itself-->
    <!--<param name="max_vel_theta" value="0.5" type="double"/>
    <param name="min_vel_theta" value="0.1" type="double"/> -->
    <param name="controller_frequency" value="10.0" type="double"/>

  <!-- Sets common_costmap parameters in config folder to both local and global costmaps -->
    <rosparam file="$(find gen2bot)/config/common_costmap.yaml" command="load" ns="global_costmap" />
    <rosparam file="$(find gen2bot)/config/common_costmap.yaml" command="load" ns="local_costmap" />
    
  <!-- Sets local and global parameters in config folder to both local and global costmaps -->
    <rosparam file="$(find gen2bot)/config/global_costmap.yaml" command="load" ns="global_costmap" /> 
    <rosparam file="$(find gen2bot)/config/local_costmap.yaml" command="load" ns="local_costmap" />

  </node>

  <!-- Launches my listenerMotorAuto.cpp file to initialize a base controller 
    that move_base communicates with to send motor inputs. This links autonomy to wheel motors -->
  <node pkg="gen2bot" type="listenerMotorAuto" name="auto_wheels" >
      <param name="initalOutput" value="0.0" type="double"/>
  </node>

  <!-- Launches node that converts xbox controller inputs into ROS-->
  <node name="joy2" pkg="joy" type="joy_node" output="screen">

    <!-- parameter that retrieves which joystick controller to link with -->
    <param name="dev" value="/dev/input/js0"/>
    <param name="default_trig_val" value="true"/>
  </node>
 
  <!-- Node that takes ROS inputs of joystick to move motors in wheels, auger, ballscrew, and linear actuator-->
  <node name="manual_motor_control" pkg="gen2bot" type="controllerInputs.py" output="screen"/>

  <!-- Node that takes ROS inputs of joystick to move motors in wheels-->
  <node name="manual_wheels" pkg="gen2bot" type="listenerMotor" output="screen"/>

  <!-- loads states.yaml's parameters-->
  <rosparam file="$(find gen2bot)/config/states.yaml" command="load"/> 

  <!--<node name="mb_client" pkg="gen2bot" type="move_base_client.py" output="screen"/>
  <node name="miningOperationsAuger" pkg="gen2bot" type="miningOperationsAuger" output="screen"/>-->


</launch>
