<launch>
<!-- Learn about launch files in ros here: http://wiki.ros.org/roslaunch/XML -->

<!-- This launch file is working towards full autonomy for Alabama's competition at the end of May-->

<!-- Currently, we need to split these nodes into different launch files so each computer can run its own nodes, not all on the jetson -->

  <!-- Launches depth camera with following parameters: 
  https://github.com/IntelRealSense/realsense-ros/tree/ros1-legacy -->

  <!-- This group ns tag merges all nodes contained within the tag into: depth_camera-->
  <group ns="depth_camera">

  <!--This file can be found in Other Locations/opt/ros/melodic/share/launch/includes on the jetson-->
    <include file="$(find realsense2_camera)/launch/includes/nodelet.launch.xml">

  <!-- Some of these parameters are in default settings, meaning we don't need to declare them, but leave
  for now as we might need to change these for our next object recognition script-->
      <arg name="depth_width"       value="640"/>
      <arg name="depth_height"      value="480"/>
      <arg name="depth_fps"         value="30"/>
      <arg name="color_width"       value="640"/>
      <arg name="color_height"      value="480"/>
      <arg name="color_fps"         value="30"/>
      <arg name="enable_fisheye"    value="false"/>
      <arg name="enable_pointcloud" value="false"/>
      <arg name="enable_sync"       value="true"/>
      <arg name="enable_gyro"       value="false"/>
      <arg name="enable_accel"      value="false"/>
      <arg name="tf_prefix"         value="camera"/>
      <arg name="align_depth"       value="true"/>
    </include>

<!-- Launches rviz using the rgbd.launch file which, somehow, allows pointcloud to work. pointcloud is what camera publishes to get
  positional values of all objects and points in space it can calculate relative to camera's frame-->
    <node name="rviz" pkg="rviz" type="rviz" args="-d $(find realsense2_camera)/rviz/pointcloud.rviz" required="true" />
      <include file="$(find gen2bot)/launch/rgbd.launch">
        <arg name="manager"                       value="realsense2_camera_manager" />
        <arg name="respawn"                       value="false" />      
        <arg name="rgb"                           value="color" />
      </include>

<!-- Remaps pointcloud so that it subscribes to a topic possible for jetson to visualize, without 
this node, jetson won't publish pointcloud, which navigation and qr code relies on -->
    <node pkg="nodelet" type="nodelet" name="points_xyzrgb_hw_registered"
      args="load depth_image_proc/point_cloud_xyzrgb realsense2_camera_manager false" respawn="false">
     
      <!-- Need these to make a pointcloud-->
      <remap from="rgb/image_rect_color"        to="color/image_rect_color" />
      <remap from="rgb/camera_info"             to="color/camera_info" />
      <remap from="depth_registered/image_rect" to="aligned_depth_to_color/image_raw" />
      <remap from="depth_registered/points"     to="/depth_camera/depth/color/points" />
    </node>
  </group>

<!-- Launches tracking camera with following parameters: 
  https://github.com/IntelRealSense/realsense-ros/tree/ros1-legacy -->
  <group ns="tracking_camera">
    <include file="$(find realsense2_camera)/launch/includes/nodelet.launch.xml">
      <!-- Initializes T265 camera-->
      <arg name="tf_prefix"                value="camera"/>
      <arg name="device_type"              value="t265"/>

      <!-- Enables IMU sensors-->
      <arg name="enable_gyro"              value="true"/>
      <arg name="enable_accel"             value="true"/>
      <arg name="enable_pose"              value="true"/>
      
      <!-- Syncs both IMU sensors into 1 sensor-->
      <arg name="unite_imu_method"         value="linear_interpolation"/>

      <!-- Sets up the global static frame "map" and gives pose estimation
      between map and camera_pose_frame (global -> odom) -->
      <arg name="publish_odom_tf"          value="true"/>
      <arg name="publish_tf"               value="true"/>
      <arg name="odom_frame_id"            value="map"/> 
    </include>
  </group>

  <!-- Object recognition package to detect QR code and display both position and orientation 
  of QR code relative to camera sensor
  Check out: https://github.com/bandasaikrishna/object_detection_and_3d_pose_estimation -->
  <!-- Will need to decide if we want to keep the gui or not, gives us camera image but takes up space and processing power-->
  <group ns = "qr_code_detector">
    <node name="find_object_3d" pkg="find_object_2d" type="find_object_2d" output="screen">
      <param name="settings_path" value="~/.ros/find_object_2d.ini" type="str"/>
      <param name="subscribe_depth" value="true" type="bool"/>

      <!-- Where to store QR code image-->
      <param name="session_path" value="$(find gen2bot)/sessions/qr.bin" type="str"/>
      
      <!-- Remap image topic to one the depth camera uses-->
      <remap from="rgb/image_rect_color" to="/depth_camera/color/image_raw"/>
      <remap from="depth_registered/image_raw" to="/depth_camera/depth/image_rect_raw"/>
      <remap from="depth_registered/camera_info" to="/depth_camera/depth/camera_info"/>
    </node>
    
    <!--<node name="tf_example" pkg="find_object_2d" type="tf_example" output="screen">
        <param name="map_frame_id" value="/camera" type="str"/>
      <param name="object_prefix" value="object" type="str"/>
    </node> -->
  </group>


  <!-- Map server used to create a map that detects obstacles check out: 
  http://wiki.ros.org/octomap_server for parameter list
  this detects obstacles for global planner's static frame: map-->
	<node pkg="octomap_server" type="octomap_server_node" name="octomap_server">

  <!-- The frame where ground level is detected-->
		<param name="base_frame_id" type="string" value="/base_footprint" />

  <!-- The frame where obstacles are embedded onto-->
		<param name="frame_id" type="string" value="/map" />

  <!-- The max range in which cameras can detect an obstacle-->
		<param name="sensor_model/max_range" value="3.0" />

  <!-- Enables detection of a ground plane, so we ignore the ground. 
  Otherwise, whole map is an obstacle-->
		<param name="filter_ground" type="bool" value="true" />

  <!-- How much in meters in Z direction to segment voxels into ground plane-->
    <param name="ground_filter/plane_distance" value="0.2" />
  
  <!-- The minimum and maximum range in which obstacles can be detected relative to camera-->
		<param name="pointcloud_min_z" value="-0.90" />
		<param name="pointcloud_max_z" value="0.10" />

  <!-- Remaps subscribed topic to topic in which pointcloud is published-->
		<remap from="cloud_in" to="/depth_camera/depth/color/points" />

  <!-- Remaps published topic to /map so map frame gets obstacles-->
   	<remap from="projected_map" to="/map"/>
	</node>

  <!-- Look at: http://docs.ros.org/en/noetic/api/robot_localization/html/index.html#:~:text=robot_localization%20is%20a%20collection%20of,estimation%20nodes%2C%20ekf_localization_node%20and%20ukf_localization_node%20.
  and https://kapernikov.com/the-ros-robot_localization-package/
  and http://docs.ros.org/en/melodic/api/robot_localization/html/state_estimation_nodes.html
  to understand parameters and purpose of robot_localization ekf -->
  <!-- Publishes base_link transform from camera_pose_frame using VIO, and move_base twist topic-->
  <node pkg="robot_localization" type="ekf_localization_node" name="ekf_localization" output="screen">
    <rosparam file="$(find gen2bot)/config/robot_localization.yaml" command="load"/>
  </node>

  <!-- Static tranform between parent and child transforms. Arguments are position for x, y, and z followed by orientation for x, y, and z -->
  <!-- These nodes just sticks a child tf to follow a parent tf a certain position and orientation away -->
  <node pkg="tf" type="static_transform_publisher" name="base_link_to_base_footprint" args= "-0.40 0.15 -0.45 0 0 0 base_link base_footprint 100"/>

  <!-- Launches move_base which spews out motor commands based on nav_goal and obstacles
    Check out: http://wiki.ros.org/move_base for parameters -->
  <node pkg="move_base" type="move_base" respawn="false" name="move_base" output="screen"> 
    
    <!-- Decrease turn velocty so robot doesn't snap into a 180 and break itself-->
    <param name="max_vel_theta" value="0.33" type="double"/>
    <param name="min_vel_theta" value="-0.33" type="double"/>
    <param name="controller_frequency" value="6.0" type="double"/>

<!-- Sets common_costmap parameters in config folder to both local and global costmaps -->
    <rosparam file="$(find gen2bot)/config/common_costmap.yaml" command="load" ns="global_costmap" />
    <rosparam file="$(find gen2bot)/config/common_costmap.yaml" command="load" ns="local_costmap" />
    
<!-- Sets local and global parameters in config folder to both local and global costmaps -->
    <rosparam file="$(find gen2bot)/config/global_costmap.yaml" command="load" ns="global_costmap" /> 
    <rosparam file="$(find gen2bot)/config/local_costmap.yaml" command="load" ns="local_costmap" />

<!-- So that it can publish to robot_localization -->
    <remap from="/odom" to="/odometry/filtered"/>

  </node>

  <!-- Launches my autoDrive.cpp file to initialize a base controller 
    that move_base communicates with to send motor inputs. This links autonomy to wheel motors -->
  <node pkg="gen2bot" type="autoDrive" name="auto_wheels" >
      <param name="initalOutput" value="0.0" type="double"/>
  </node>

  <!-- Launches my move_base_client.py script to send desired positions for robot to reach to move_base planner-->
  <node name="move_baseClient" pkg="gen2bot" type="move_base_client.py" output="screen">
  </node>

  <!-- Launches node that converts xbox controller inputs into ROS; 
  http://wiki.ros.org/joy          http://wiki.ros.org/joy/Tutorials/ConfiguringALinuxJoystick -->
  <node name="joy2" pkg="joy" type="joy_node" output="screen">

    <!-- parameter that retrieves which joystick controller to link with -->
    <param name="dev" value="/dev/input/js0"/>

    <!-- Makes triggers work on bootup-->
    <param name="default_trig_val" value="true"/>
  </node>
 
  <!-- Node that takes ROS inputs of joystick to move motors in wheels, auger, ballscrew, and linear actuator-->
  <node name="manual_motor_control" pkg="gen2bot" type="controllerInputs.py" output="screen">
  </node>

  <!-- Takes in twist messages and converts them to twistWithCovariancePose messages so robot_localization can use them-->
  <node name="twist_convertor" pkg="gen2bot" type="twistConvertor.py" output="screen">
  </node>

  <!-- Node that takes ROS inputs of joystick to move motors in wheels-->
  <node name="manual_wheels" pkg="gen2bot" type="manualDrive" output="screen">
  </node>

  <!-- Node that contains all motor functions that don't relate to driving -->
  <node pkg="gen2bot" type="miningOperationsTrencherAuto" name="mining_operations" output="screen">
    <param name="wheel_multiplier" value="1.1" type="double"/>
    <rosparam file="$(find gen2bot)/config/PID.yaml" command="load"/> 
  </node> 

  <!-- loads states.yaml's parameters-->
  <rosparam file="$(find gen2bot)/config/states.yaml" command="load"/> 

 <node pkg="gen2bot" type="move_base_client.py" name="move_base_client.py" output="screen">
 </node> 

</launch>
